from datetime import timedelta
from datetime import datetime
import pandas as pd
from os import remove
from catboost import CatBoostRegressor as cbr
import joblib

from airflow import DAG
from airflow.operators.python import PythonOperator

from airflow.providers.amazon.aws.hooks.s3 import S3Hook

import logging
#from news_parser import Scrapper
#from Parsing_index import SPBIRUS2DataDownloader, SPBEDataDownloader
from airflow.utils.dates import days_ago
from botocore.exceptions import (
    ConnectTimeoutError,
    EndpointConnectionError,
    ConnectionError,
)

_LOG = logging.getLogger()
_LOG.addHandler(logging.StreamHandler())

cid = "s3_connection"
s3_hook = S3Hook(cid)

DEFAULT_ARGS = {
    "owner": "Team 22",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 3,
    "retry_delay": timedelta(minutes=1)
}

dag = DAG(
    "spbi_dag",
    tags=["mlops"],
    catchup=False,
    start_date=days_ago(2),
    default_args=DEFAULT_ARGS,
    schedule_interval="@once",
)

def init():
    """
    Step0: Pipeline initialisation.
    """
    info = {}
    info["start_timestamp"] = datetime.now().strftime("%Y%m%d %H:%M")
    # Импользуем данные с сегодня на 2 года назад.
    info["date_start"] = datetime.now().strftime("%Y-%m-%d")
    info["date_end"] = datetime.now().strftime("%Y-%m-%d")
    return info

def query_forming_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="init")
    info["query_spbi_forming_start"] = datetime.now().strftime("%Y%m%d %H:%M")

    max_news_count = 20
    lookback = 65
    
    try:
        remove('data/finam_news_scored.csv')
    except:
        pass
    s3_hook.download_file(
        key = "finam_news_scored.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    news = pd.read_csv("data/finam_news_scored.csv")

    try:
        remove('data/SPBIRUS2_filled.csv')
    except:
        pass
    s3_hook.download_file(
        key = "SPBIRUS2_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/SPBIRUS2_filled.csv")
    remove('data/finam_news_scored.csv')
    remove('data/SPBIRUS2_filled.csv')
    
    news["day"] = pd.to_datetime(news["day"])
    cur_date = news.iloc[-1].day

    query = pd.DataFrame()
    news_count = news[news.day == cur_date].shape[0]
    query["news_size"] = [news_count]
    next_date = cur_date+timedelta(1)
    query["year"] = [next_date.year]
    query["month"] = [next_date.month]
    query["day"] = [next_date.day]
    

    res = news[news.day == cur_date].sample(min(max_news_count, news_count)).score

    for i in range(max_news_count - min(max_news_count, news_count)):
        res = res.append(pd.Series([0.0]), ignore_index=True)

    res = res.reset_index(drop=True)

    for i in range(max_news_count):
        query[f"news{i}"] = res[i]

    lags = df_filled[df_filled.date.between(str(cur_date-timedelta(days=lookback)),str(cur_date))].open.reset_index(drop=True)
    
    for i in range(1,lookback+1):
        query["shift"+str(i)] = lags[lags.size-i]
    
    info["query_spbi"] = query
    _LOG.info("Query forming finished.")
    info["query_spbi_forming_end"] = datetime.now().strftime("%Y%m%d %H:%M")

    return info

def retrain_model_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="query_forming_spbi")
    info["retraining_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M") 
    
    try:
        remove('data/for_catboost_spbi.csv')
    except:
        pass
    s3_hook.download_file(
        key = "for_catboost_spbi.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )

    df = pd.read_csv('data/for_catboost_spbi.csv')
    
    try:
        remove('data/yesterday_query_spbi.csv')
    except:
        pass
    s3_hook.download_file(
        key = "yesterday_query_spbi.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    params = {'bootstrap_type':'MVS','depth':4,'l2_leaf_reg':1,'learning_rate': 0.058264711668021676, 'min_data_in_leaf':81}
    q = pd.read_csv('data/yesterday_query_spbi.csv')
    df = pd.concat([df, q], ignore_index = True)
    cb_reg = cbr(**params)

    try:
        remove('data/SPBIRUS2_filled.csv')
    except:
        pass
    s3_hook.download_file(
        key = "SPBIRUS2_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    spbi_filled = pd.read_csv('data/SPBIRUS2_filled.csv')

    X = df.drop(columns=['open'])
    y = df['open']
    cb_reg.fit(X,y)
    joblib.dump(cb_reg, "data/catboost_spbi.joblib")
    s3_hook.load_file("data/catboost_spbi.joblib", key = "catboost_spbi.joblib", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/catboost_spbi.joblib')

    today_query = info["query_spbi"]
    today_query['open'] = spbi_filled.tail(1).open.values[0]
    today_query.to_csv('data/today_query.csv',index=False)
    s3_hook.load_file('data/today_query.csv', key = "yesterday_query_spbi.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/for_catboost_spbi.csv')
    remove('data/yesterday_query_spbi.csv')
    remove('data/SPBIRUS2_filled.csv')

    _LOG.info("Retrain SPBIRUS2 finished.")
    info["retraining_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info



def model_predict_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="retrain_model_spbi")
    info["prediction_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M")
    
    try:
        remove('data/catboost_spbi.joblib')
    except:
        pass
    s3_hook.download_file(
        key = "catboost_spbi.joblib",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    model = joblib.load("data/catboost_spbi.joblib")
    query = info["query_spbi"]
    preds = model.predict(query)
    preds = pd.DataFrame({'date':(datetime.strptime(info["date_start"], '%Y-%m-%d')+timedelta(1)), 'open': preds})
    params = model.get_all_params()
    info["cb_params_spbi"] = params
    preds.to_csv('data/catboost_preds_spbi.csv', index = False)


    s3_hook.load_file('data/catboost_preds_spbi.csv', key = "prediction_spbi.csv", 
                        bucket_name="studcamp-ml", replace=True)
    try:
        remove('data/catboost.joblib')
    except:
        pass
    try:
        remove('data/catboost_preds_spbi.csv')
    except:
        pass
    info["predict_spbi"] = preds
    _LOG.info("Prediction_spbi_finished.")
    info["prediction_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

def save_last_hundred_spbi(**kwargs):
    ti = kwargs["ti"]
    info = ti.xcom_pull(task_ids="model_predict_spbi")
    info["save_last_spbi_start"] = datetime.now().strftime("%Y%m%d %H:%M")   

    try:
        remove('data/SPBIRUS2_filled.csv')
    except:
        pass
    s3_hook.download_file(
        key = "SPBIRUS2_filled.csv",
        bucket_name = "studcamp-ml",
        local_path = 'data',
        preserve_file_name = True,
        use_autogenerated_subdir = False
    )
    df_filled = pd.read_csv("data/SPBIRUS2_filled.csv").tail(99)
    preds = info["predict_spbi"]
    last_hundred = pd.concat([df_filled, preds], ignore_index=True)
    last_hundred.to_csv('data/last_hundred_spbi.csv', index = False)

    s3_hook.load_file('data/last_hundred_spbi.csv', key = "last_hundred_spbirus2.csv", 
                        bucket_name="studcamp-ml", replace=True)
    remove('data/SPBIRUS2_filled.csv')
    remove('data/last_hundred_spbi.csv')
    _LOG.info("Save last spbi finished.")
    info["save_last_spbi_end"] = datetime.now().strftime("%Y%m%d %H:%M")
    return info

t1 = PythonOperator(
    task_id="init",
    provide_context=True,
    python_callable=init,
    dag=dag
)

t2 = PythonOperator(
    task_id="query_forming_spbi",
    provide_context=True,
    python_callable=query_forming_spbi,
    dag=dag
)

t3 = PythonOperator(
    task_id="retrain_model_spbi",
    provide_context=True,
    python_callable=retrain_model_spbi,
    dag=dag
)

t4 = PythonOperator(
    task_id="model_predict_spbi",
    provide_context=True,
    python_callable=model_predict_spbi,
    dag=dag
)

t5 = PythonOperator(
    task_id="save_last_hundred_spbi",
    provide_context=True,
    python_callable=save_last_hundred_spbi,
    dag=dag
)

t1 >> t2 >> t3 >> t4 >> t5